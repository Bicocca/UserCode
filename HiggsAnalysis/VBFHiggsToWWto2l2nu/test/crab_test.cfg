[CRAB]
jobtype = cmssw
scheduler = glite
server_name = bari

[CMSSW]

### The data you want to access (to be found on DBS)

#datasetpath=/H180_ZZ_2l2nu/Summer08_IDEAL_V9_v1/GEN-SIM-RECO
#datasetpath=/H180_WW_2l/CMSSW_1_6_7-CSA07-1192835387/RECO
datasetpath=/H500_WW_2l/Summer08_IDEAL_V9_v2/GEN-SIM-RECO

### The ParameterSet you want to use
pset=VBFHWW2l2nuSkim_cfg.py

### Splitting parameters
total_number_of_events=-1
#total_number_of_events=100
events_per_job = 5000
#number_of_jobs = 1

### The output files (comma separated list)
output_file = VBFHWW2l2nuTest.root

[USER] 
copy_data = 1
storage_element = srm-cms.cern.ch
storage_path = /srm/managerv2?SFN=/castor/cern.ch
user_remote_dir=/user/a/amassiro/VBF/Skimmed/H500_WW_2l_Summer08_IDEAL_V9_v2/



### OUTPUT files Management
##  output back into UI
return_data = 0

#if server
#thresholdLevel = 80
#eMail = andrea.massironi@cern.ch

[EDG]

## RB/WMS management:
rb = CERN

##  Black and White Lists management:
## By Storage
se_black_list = T0
#se_white_list =

## By ComputingElement
#ce_black_list =
#ce_white_list =

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))

